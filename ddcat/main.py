import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import argparse
import os
import sys
import evaluate

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (relative import í•´ê²°)
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(current_dir)
sys.path.append(parent_dir)

# ddcat ëª¨ë¸ë“¤ import
from model import DeepLabV3, DeepLabV3_DDCAT, PSPNet, PSPNet_DDCAT

# ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ dataset í´ë˜ìŠ¤ë“¤ import
from dataset import CitySet, ADESet, VOCSet

def load_config(dataset_type, model_type):
    """configs_attackì—ì„œ ì„¤ì • ë¡œë“œ"""
    config_map = {
        'deeplabv3': 'config_deeplabv3',
        'deeplabv3_ddcat': 'config_deeplabv3',
        'pspnet': 'config_pspnet', 
        'pspnet_ddcat': 'config_pspnet'
    }
    
    config_name = config_map.get(model_type, 'config_deeplabv3')
    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'configs_attack', dataset_type, f'{config_name}.py'
    )
    
    if not os.path.exists(config_path):
        print(f"ê²½ê³ : ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return None
    
    # config íŒŒì¼ì„ ë™ì ìœ¼ë¡œ import
    import importlib.util
    spec = importlib.util.spec_from_file_location("config_module", config_path)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    
    return config_module.config

class DDCATInferenceWithDataset:
    def __init__(self, model_type='deeplabv3_ddcat', model_path=None, device='cuda'):
        """
        DDCAT ëª¨ë¸ì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ì¶”ë¡  ë° mIoU ê³„ì‚° í´ë˜ìŠ¤
        
        Args:
            model_type (str): ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ('deeplabv3', 'deeplabv3_ddcat', 'pspnet', 'pspnet_ddcat')
            model_path (str): í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
            device (str): ì¶”ë¡ ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = self._load_model(model_type)
        self.model.to(self.device)
        self.model.eval()
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if model_path and os.path.exists(model_path):
            self._load_weights(model_path)
        else:
            print("ì‚¬ì „í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
        # ì „ì²˜ë¦¬ ë³€í™˜ (BGR -> RGB -> Tensor -> Normalize)
        # dataset.pyëŠ” BGR í˜•íƒœë¡œ ë°˜í™˜í•˜ë¯€ë¡œ RGBë¡œ ë³€í™˜ í•„ìš”
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # mIoU ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.miou_calculator = evaluate.load("mean_iou")
        
    def _load_model(self, model_type):
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ëª¨ë¸ ì´ˆê¸°í™”"""
        if model_type == 'deeplabv3':
            return DeepLabV3(layers=50, classes=2, pretrained=True)
        elif model_type == 'deeplabv3_ddcat':
            return DeepLabV3_DDCAT(layers=50, classes=2, pretrained=True)
        elif model_type == 'pspnet':
            return PSPNet(layers=50, classes=2, pretrained=True)
        elif model_type == 'pspnet_ddcat':
            return PSPNet_DDCAT(layers=50, classes=2, pretrained=True)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    def _load_weights(self, model_path):
        """í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'])
            elif 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'])
            else:
                self.model.load_state_dict(checkpoint)
            print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_path}")
        except Exception as e:
            print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def preprocess_image_from_dataset(self, bgr_image, target_size=(512, 512)):
        """dataset.pyì—ì„œ ê°€ì ¸ì˜¨ BGR ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # BGR -> RGB ë³€í™˜
        if len(bgr_image.shape) == 3:
            rgb_image = bgr_image[:, :, ::-1]  # BGR to RGB
        else:
            rgb_image = bgr_image
            
        # PIL Imageë¡œ ë³€í™˜
        image = Image.fromarray(rgb_image.astype(np.uint8))
        
        # ì›ë³¸ í¬ê¸° ì €ì¥
        original_size = image.size
        
        # í¬ê¸° ì¡°ì • (8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • - ëª¨ë¸ ìš”êµ¬ì‚¬í•­)
        width, height = target_size
        width = ((width - 1) // 8 + 1) * 8 + 1
        height = ((height - 1) // 8 + 1) * 8 + 1
        
        image = image.resize((width, height), Image.BILINEAR)
        
        # í…ì„œë¡œ ë³€í™˜ ë° ì •ê·œí™”
        image_tensor = self.transform(image).unsqueeze(0)
        
        return image_tensor, original_size, (width, height)
    
    def postprocess_output(self, output, original_size, processed_size):
        """ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬"""
        if isinstance(output, tuple):
            # training ëª¨ë“œì—ì„œì˜ ì¶œë ¥ì¸ ê²½ìš°
            output = output[0] if len(output) > 0 else output
            
        # logitsë¥¼ í™•ë¥ ë¡œ ë³€í™˜
        if output.dim() == 4:  # (batch, classes, height, width)
            probabilities = torch.softmax(output, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
        else:
            predictions = output
            
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        if predictions.dim() == 3:
            predictions = predictions.squeeze(0)
        
        # CPUë¡œ ì´ë™í•˜ê³  numpyë¡œ ë³€í™˜
        predictions = predictions.cpu().numpy().astype(np.uint8)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        if original_size != processed_size:
            predictions = cv2.resize(predictions, original_size, interpolation=cv2.INTER_NEAREST)
        
        return predictions
    
    def load_dataset(self, dataset_type, dataset_dir):
        """ë°ì´í„°ì…‹ ë¡œë“œ"""
        if dataset_type.lower() == 'cityscapes':
            return CitySet(dataset_dir, use_gt=True)
        elif dataset_type.lower() == 'ade20k':
            return ADESet(dataset_dir, use_gt=True)
        elif dataset_type.lower() == 'voc2012':
            return VOCSet(dataset_dir, use_gt=True)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_type}")
    
    def calculate_miou(self, predictions, ground_truths, num_classes, ignore_index=255, reduce_labels=False):
        """mIoU ê³„ì‚°"""
        try:
            miou_result = self.miou_calculator.compute(
                predictions=predictions,
                references=ground_truths,
                num_labels=num_classes,
                ignore_index=ignore_index,
                reduce_labels=reduce_labels
            )
            return miou_result
        except Exception as e:
            print(f"mIoU ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def evaluate_dataset(self, dataset_type, dataset_dir=None, target_size=(512, 512), num_classes=None, max_samples=None, config=None):
        """ë°ì´í„°ì…‹ ì „ì²´ í‰ê°€ ë° mIoU ê³„ì‚°"""
        
        # configê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ìë™ ë¡œë“œ
        if config is None:
            config = load_config(dataset_type, self.model_type)
        
        # configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        if config:
            print(f"configs_attackì—ì„œ ì„¤ì • ë¡œë“œ: {config}")
            if dataset_dir is None:
                dataset_dir = config.get('data_dir', f'datasets/{dataset_type}')
            if num_classes is None:
                num_classes = config.get('num_class', 2)
            print(f"ì„¤ì •ëœ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
            print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {dataset_dir}")
        else:
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            if dataset_dir is None:
                dataset_dir = f'datasets/{dataset_type}'
            if num_classes is None:
                if dataset_type.lower() == 'cityscapes':
                    num_classes = 19
                elif dataset_type.lower() == 'ade20k':
                    num_classes = 150
                elif dataset_type.lower() == 'voc2012':
                    num_classes = 21
                else:
                    num_classes = 2  # ê¸°ë³¸ê°’
        
        # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not os.path.exists(dataset_dir):
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_dir}")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        print(f"=== {dataset_type.upper()} ë°ì´í„°ì…‹ í‰ê°€ ì‹œì‘ ===")
        print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {dataset_dir}")
        dataset = self.load_dataset(dataset_type, dataset_dir)
        print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ ì´ë¯¸ì§€")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        total_samples = len(dataset)
        if max_samples and max_samples < total_samples:
            total_samples = max_samples
            print(f"ìƒ˜í”Œ ìˆ˜ë¥¼ {max_samples}ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
        
        all_predictions = []
        all_ground_truths = []
        
        for i in range(total_samples):
            print(f"[{i+1}/{total_samples}] ì²˜ë¦¬ ì¤‘...")
            
            try:
                # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ì§€ì™€ GT ê°€ì ¸ì˜¤ê¸°
                bgr_image, filename, gt_image = dataset[i]
                
                print(f"  íŒŒì¼: {filename}")
                print(f"  ì´ë¯¸ì§€ í¬ê¸°: {bgr_image.shape}")
                print(f"  GT í¬ê¸°: {gt_image.shape}")
                
                # ì „ì²˜ë¦¬
                image_tensor, original_size, processed_size = self.preprocess_image_from_dataset(
                    bgr_image, target_size
                )
                image_tensor = image_tensor.to(self.device)
                
                # ì¶”ë¡ 
                with torch.no_grad():
                    output = self.model(image_tensor)
                
                # í›„ì²˜ë¦¬
                prediction = self.postprocess_output(output, original_size, processed_size)
                
                # Ground Truth í¬ê¸° ì¡°ì • (ì˜ˆì¸¡ ê²°ê³¼ì™€ ë§ì¶¤)
                if gt_image.shape[:2] != prediction.shape:
                    gt_resized = cv2.resize(gt_image, 
                                          (prediction.shape[1], prediction.shape[0]), 
                                          interpolation=cv2.INTER_NEAREST)
                else:
                    gt_resized = gt_image
                
                all_predictions.append(prediction)
                all_ground_truths.append(gt_resized)
                
                print(f"  ì™„ë£Œ - ì˜ˆì¸¡: {prediction.shape}, GT: {gt_resized.shape}")
                
            except Exception as e:
                print(f"  ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        # mIoU ê³„ì‚°
        if all_predictions:
            print(f"\n=== {len(all_predictions)}ê°œ ì´ë¯¸ì§€ mIoU ê³„ì‚° ===")
            
            # ë°ì´í„°ì…‹ë³„ reduce_labels ì„¤ì •
            reduce_labels = (dataset_type.lower() == 'ade20k')
            
            miou_result = self.calculate_miou(
                all_predictions, all_ground_truths, 
                num_classes, reduce_labels=reduce_labels
            )
            
            if miou_result:
                print(f"ì „ì²´ mIoU: {miou_result['mean_iou']:.4f}")
                print(f"í´ë˜ìŠ¤ë³„ IoU:")
                for class_idx, iou in enumerate(miou_result['per_class_iou']):
                    if not np.isnan(iou):
                        print(f"  í´ë˜ìŠ¤ {class_idx}: {iou:.4f}")
                
                return miou_result
            else:
                print("mIoU ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return None
        else:
            print("ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
def main():
    parser = argparse.ArgumentParser(description='DDCAT ëª¨ë¸ì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ í‰ê°€ ë° mIoU ê³„ì‚° (configs_attack ì„¤ì • ì‚¬ìš©)')
    parser.add_argument('--dataset_type', type=str, required=True,
                      choices=['cityscapes', 'ade20k', 'VOC2012'],
                      help='ë°ì´í„°ì…‹ íƒ€ì…')
    parser.add_argument('--dataset_dir', type=str,
                      help='ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: configs_attackì—ì„œ ìë™ ì„¤ì •)')
    parser.add_argument('--model_type', type=str, default='deeplabv3_ddcat',
                      choices=['deeplabv3', 'deeplabv3_ddcat', 'pspnet', 'pspnet_ddcat'],
                      help='ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…')
    parser.add_argument('--model_path', type=str, help='í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'],
                      help='ì¶”ë¡ ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤')
    parser.add_argument('--target_size', type=int, nargs=2, default=[512, 512],
                      help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (width height)')
    parser.add_argument('--num_classes', type=int, help='í´ë˜ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: configs_attackì—ì„œ ìë™ ì„¤ì •)')
    parser.add_argument('--max_samples', type=int, help='í‰ê°€í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ì œí•œ)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ DDCAT ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print(f"   ë°ì´í„°ì…‹: {args.dataset_type}")
    print(f"   ëª¨ë¸: {args.model_type}")
    
    # configs_attackì—ì„œ ì„¤ì • ë¡œë“œ
    config = load_config(args.dataset_type, args.model_type)
    if config:
        print(f"âœ… configs_attackì—ì„œ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"âš ï¸  configs_attack ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    try:
        # ì¶”ë¡  ê°ì²´ ìƒì„±
        evaluator = DDCATInferenceWithDataset(
            model_type=args.model_type,
            model_path=args.model_path,
            device=args.device
        )
        
        # ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰ (config ì „ë‹¬)
        miou_result = evaluator.evaluate_dataset(
            dataset_type=args.dataset_type,
            dataset_dir=args.dataset_dir,  # Noneì¼ ìˆ˜ ìˆìŒ (configì—ì„œ ìë™ ì„¤ì •)
            target_size=tuple(args.target_size),
            num_classes=args.num_classes,  # Noneì¼ ìˆ˜ ìˆìŒ (configì—ì„œ ìë™ ì„¤ì •)
            max_samples=args.max_samples,
            config=config
        )
        
        if miou_result:
            print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
            print(f"   mIoU: {miou_result['mean_iou']:.4f}")
            print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {args.model_type}")
            print(f"   ë°ì´í„°ì…‹: {args.dataset_type}")
            if config:
                print(f"   ì„¤ì • íŒŒì¼: configs_attack/{args.dataset_type}/config_{args.model_type.replace('_ddcat', '')}.py")
        else:
            print("âŒ í‰ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
