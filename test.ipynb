{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def convert_to_train_id(label_array):\n",
    "    \"\"\"\n",
    "    Cityscapes의 원본 레이블을 학습에 사용되는 레이블로 변환합니다.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5, 19: 6, 20: 7,\n",
    "        21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "        28: 15, 31: 16, 32: 17, 33: 18\n",
    "    }\n",
    "    return np.vectorize(lambda x: mapping.get(x, 255))(label_array)\n",
    "\n",
    "def label_to_train_id(gt_mask):\n",
    "    gt = np.array(gt_mask) if isinstance(gt_mask, Image.Image) else np.array(gt_mask)\n",
    "    gt = convert_to_train_id(gt)\n",
    "    return gt\n",
    "\n",
    "def save_results(results, filename):\n",
    "    \"\"\"\n",
    "    결과를 JSON 파일에 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        results: 저장할 결과 딕셔너리\n",
    "        filename: 저장할 파일 이름\n",
    "    \"\"\"\n",
    "    \n",
    "    Result_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"Result\")\n",
    "    if not os.path.exists(Result_dir):\n",
    "        os.makedirs(Result_dir)\n",
    "    file_path = os.path.join(Result_dir, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            # 기존 파일이 리스트 형식이 아닐 경우 리스트로 감싸줍니다.\n",
    "            existing_results = json.load(f)\n",
    "            if not isinstance(existing_results, list):\n",
    "                existing_results = [existing_results]\n",
    "    else:\n",
    "        existing_results = []\n",
    "    \n",
    "    existing_results.append(results)\n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(existing_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"결과가 '{file_path}' 파일에 추가 저장되었습니다.\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred, metric, num_labels):\n",
    "    pred, labels = eval_pred\n",
    "    \n",
    "    # pred가 이미 최종 예측 결과인 경우 (클래스 인덱스)\n",
    "    # 메트릭 계산\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred,\n",
    "        references=labels,\n",
    "        num_labels=num_labels,\n",
    "        ignore_index=255,\n",
    "        reduce_labels=False,\n",
    "    )\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from torchvision.datasets import Cityscapes\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# 설정값을 클래스로 정의하여 관리\n",
    "class AttackConfig:\n",
    "    model_name = \"shi-labs/oneformer_cityscapes_swin_large\"\n",
    "    data = \"cityscapes\"\n",
    "    DataSize = 500\n",
    "    batch_size = 10\n",
    "    Dataset = \"val\"\n",
    "\n",
    "def infer_full_image(image, processor, model, device, split_size=(512, 1024)):\n",
    "    \"\"\"\n",
    "    전체 이미지에 대한 추론을 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        image: 입력 이미지 (PIL Image)\n",
    "        processor: 이미지 전처리기\n",
    "        model: 세그멘테이션 모델\n",
    "        device: 연산 장치 (CPU/GPU)\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: 세그멘테이션 결과 (height, width)\n",
    "    \"\"\"\n",
    "    # 원본 이미지 크기 확인\n",
    "    width, height = image.size\n",
    "    split_height, split_width = split_size\n",
    "    \n",
    "    # 결과를 저장할 배열 초기화\n",
    "    result = np.zeros((height, width), dtype=np.int64)\n",
    "    \n",
    "    # 이미지를 4등분하여 처리\n",
    "    for y in range(0, height, split_height):\n",
    "        for x in range(0, width, split_width):\n",
    "            # 이미지 조각 추출\n",
    "            x_end = min(x + split_width, width)\n",
    "            y_end = min(y + split_height, height)\n",
    "            \n",
    "            # 이미지 조각 생성\n",
    "            tile = image.crop((x, y, x_end, y_end))\n",
    "            \n",
    "            # 모델 추론\n",
    "            inputs = processor(images=tile, task_inputs=[\"semantic\"], return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            # 후처리\n",
    "            tile_result = processor.post_process_semantic_segmentation(\n",
    "                outputs, target_sizes=[(tile.size[1], tile.size[0])])[0]\n",
    "            \n",
    "            # 결과를 numpy 배열로 변환\n",
    "            tile_result = tile_result.cpu().numpy().astype(np.int64)\n",
    "            \n",
    "            # 결과 배열에 조각 결과 삽입\n",
    "            result[y:y_end, x:x_end] = tile_result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `OneFormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:   0%|          | 0/50 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "batch: 100%|██████████| 50/50 [2:45:48<00:00, 198.98s/it]  \n"
     ]
    }
   ],
   "source": [
    "config = AttackConfig()\n",
    "\n",
    "# Cityscapes 데이터셋 (fine annotation) 사용\n",
    "dataset = Cityscapes(root=f\"./workspace/DataSet/{config.data}/\", split=config.Dataset, mode=\"fine\", target_type=\"semantic\")\n",
    "selected_indices = list(range(len(dataset)))\n",
    "\n",
    "# 모델 로드\n",
    "processor = OneFormerProcessor.from_pretrained(config.model_name)\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(config.model_name)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 평가 준비\n",
    "num_classes = 19\n",
    "matrix = evaluate.load(\"mean_iou\")\n",
    "miou_metrics_list = []\n",
    "# 배치 처리 및 평가\n",
    "for batch_start in tqdm(range(0, len(selected_indices), config.batch_size), desc=\"batch\"):\n",
    "    batch_indices = selected_indices[batch_start:batch_start+config.batch_size]\n",
    "    for idx in tqdm(batch_indices, desc=\"image\", leave=False):\n",
    "        image, gt_mask = dataset[idx]\n",
    "        pred = infer_full_image(image, processor, model, device)\n",
    "        \n",
    "        # GT 마스크 전처리\n",
    "        gt = label_to_train_id(gt_mask)\n",
    "        metrics = compute_metrics([pred, gt], matrix, num_classes)\n",
    "        miou_metrics_list.append(metrics[\"mean_iou\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6742265417813148\n"
     ]
    }
   ],
   "source": [
    "miou_metrics = np.array(miou_metrics_list)\n",
    "print(np.mean(miou_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miou 값이 70% 이상인 이미지 수: 39\n",
      "전체 이미지 중 비율: 7.80%\n"
     ]
    }
   ],
   "source": [
    "# miou 값이 70% 이상인 수 계산\n",
    "miou_above_70 = np.sum(np.array(miou_metrics_list) >= 0.8)\n",
    "print(f\"miou 값이 70% 이상인 이미지 수: {miou_above_70}\")\n",
    "print(f\"전체 이미지 중 비율: {miou_above_70 / len(miou_metrics_list) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'segmentation_results.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(project_root)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 클래스별 평균 IoU를 시각화\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msegmentation_results.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m     class_stats \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_statistics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     class_indices \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'segmentation_results.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"./workspace/\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# 클래스별 평균 IoU를 시각화\n",
    "with h5py.File('segmentation_results.h5', 'r') as f:\n",
    "    class_stats = f['class_statistics']\n",
    "    \n",
    "    class_indices = []\n",
    "    iou_values = []\n",
    "    \n",
    "    for class_idx in range(19):\n",
    "        class_key = str(class_idx)\n",
    "        if class_key in class_stats:\n",
    "            class_indices.append(class_idx)\n",
    "            iou_values.append(class_stats[class_key].attrs['avg_iou'])\n",
    "    \n",
    "    # 막대 그래프로 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(class_indices, iou_values)\n",
    "    plt.xlabel('클래스 인덱스')\n",
    "    plt.ylabel('평균 IoU')\n",
    "    plt.title('클래스별 평균 IoU')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(class_indices)\n",
    "    plt.show()\n",
    "    \n",
    "    # 픽셀 수 대비 IoU 산점도\n",
    "    gt_pixels = []\n",
    "    for class_idx in range(19):\n",
    "        class_key = str(class_idx)\n",
    "        if class_key in class_stats:\n",
    "            gt_pixels.append(class_stats[class_key].attrs['total_gt_pixels'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(gt_pixels, iou_values)\n",
    "    plt.xscale('log')  # 픽셀 수는 로그 스케일로 표시\n",
    "    plt.xlabel('GT 픽셀 수 (로그 스케일)')\n",
    "    plt.ylabel('평균 IoU')\n",
    "    plt.title('클래스별 픽셀 수 대비 IoU')\n",
    "    \n",
    "    # 데이터 포인트에 클래스 인덱스 표시\n",
    "    for i, class_idx in enumerate(class_indices):\n",
    "        plt.annotate(str(class_idx), (gt_pixels[i], iou_values[i]))\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
